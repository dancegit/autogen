Page: modal.com_files/modal.com/docs/guide/vercel-integration.html
----------------------------------------
   [1]Modal logo
   [2]Guide [3]Examples [4]Reference [5]Playground
   [6]Log In [7]Sign Up
   (BUTTON)
   [8]Introduction[9]Custom container images [10]Custom
   containers[11]Private registries[12]GPUs and other resources [13]GPU
   acceleration[14]Using CUDA on Modal[15]Reserving CPU and
   memory[16]Scaling out [17]Scaling out[18]Dicts and queues[19]Job
   processing[20]Concurrent inputs on a single container (beta)[21]Dynamic
   batching (beta)[22]Scheduling and cron jobs[23]Deployment [24]Apps,
   Stubs, and entrypoints[25]Managing deployments[26]Invoke deployed
   functions[27]Continuous deployment[28]Secrets and environment variables
   [29]Secrets[30]Environment variables[31]Web endpoints [32]Web
   endpoints[33]Streaming endpoints[34]Web endpoint URLs[35]Request
   timeouts[36]Networking [37]Tunnels (beta)[38]Proxies (beta)[39]Data
   sharing and storage [40]Passing local data[41]Volumes[42]Mounting local
   files and directories[43]Storing model weights[44]Dataset
   ingestion[45]Cloud bucket mounts[46]Network file systems
   (superseded)[47]Sandboxes [48]Sandboxes[49]Running
   commands[50]Networking and security[51]File access[52]Performance
   [53]Cold start performance[54]Memory Snapshot (beta)[55]Geographic
   latency[56]Reliability and robustness [57]Failures and
   retries[58]Preemption[59]Timeouts[60]Troubleshooting[61]Security and
   privacy[62]Integrations [63]Connecting Modal to your Vercel
   account[64]Connecting Modal to your Datadog account[65]Connecting Modal
   to your OpenTelemetry provider[66]Okta SSO[67]Slack notifications
   (beta)[68]Other topics [69]File and project structure[70]Developing and
   debugging[71]Modal user account
   setup[72]Workspaces[73]Environments[74]Jupyter
   notebooks[75]Asynchronous API usage[76]Global variables[77]Region
   selection[78]Container lifecycle hooks[79]Parameterized functions[80]S3
   Gateway endpoints
     __________________________________________________________________

Connecting Modal to your Vercel account

   You can use the Modal + Vercel integration to access Modal's
   [81]Instant Endpoints from Vercel projects. You'll find the Modal
   Vercel integration available for install in the Vercel AI marketplace.

What this integration does

   This integration allows you to:
       projects
       projects

Authentication

   The integration will set the following environment variables against
   the user's selected Vercel projects:
     * MODAL_TOKEN_ID (starts with ak-*)
     * MODAL_TOKEN_SECRET (starts with as-*)

   The environment variables will be set in the "preview" and "production"
   project targets. You can read more about environment variables within
   Vercel [83]in the documentation.

Installing the integration

       new Modal workspace
       workspace
       were added by going to your Vercel project > "Settings" >
       "Environment variables"

Uninstalling the integration

   The Modal Vercel integration is managed under the user's Vercel
   dashboard under the "Integrations" tab. From there they can remove the
   specific integration installation from their Vercel account.

   Important: removing an integration will delete the corresponding API
   token set by Modal in your Vercel project(s).
     __________________________________________________________________

Modal Instant Endpoints

   Instant Endpoints are a fast and scalable API for integrating
   open-source AI models into your Vercel app.

   All available endpoints are listed below, along with example code
   suitable for use with the Javascript fetch API.

Stable Diffusion XL

     [85]https://modal-labs--instant-stable-diffusion-xl.modal.run

   Stable Diffusion is a latent text-to-image diffusion model able to
   generate photo-realistic images given any text prompt.

   This endpoint uses a fast version of [86]Stable Diffusion XL to create
   variably sized images up to 1024h x 1024w.

Example code

// pages/api/modal.ts
const requestData = {
  prompt: "need for speed supercar. unreal engine",
  width: 768,
  height: 768,
  num_outputs: 1,
};
const result = await fetch(
  "https://modal-labs--instant-stable-diffusion-xl.modal.run/v1/inference",
  {
    headers: {
      Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TO
KEN_SECRET}`,
      "Content-Type": "application/json",
    },
    method: "POST",
    body: JSON.stringify(requestData),
  },
);
const imageData = await result.blob();

   (BUTTON) Copy

Input schema

     * prompt string
          + Input prompt
     * height integer
          + Height of generated image in pixels. Needs to be a multiple of
            64
          + One of: 64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704,
            768, 832, 896, 960, 1024
          + Default: 768
     * width integer
          + Width of generated image in pixels. Needs to be a multiple of
            64
          + One of: 64, 128, 192, 256, 320, 384, 448, 512, 576, 640, 704,
            768, 832, 896, 960, 1024
          + Default: 768

Output schema

   This endpoint outputs bytes for a single image with [87]media type
   "image/png".

Pricing

   Requests to this endpoint use duration based pricing, billed at 1ms
   granularity. The exact cost per millisecond is based on the underlying
   GPU hardware. This endpoint use a single NVIDIA A10G device to serve
   each request.

   See our [88]pricing page for current GPU prices.

   Inferences usually complete within 15-30 seconds.

Transcribe speech with vaibhavs10/insanely-fast-whisper

   This endpoint hosts [89]vaibhavs10/insanely-fast-whisper to transcribe
   and diarize audio.

Example code

// pages/api/modal.ts
const data = {
  audio: dataUrl,
  diarize_audio: false,
};

const response = await fetch("https://modal-labs--instant-whisper.modal.run", {
  headers: {
    Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKE
N_SECRET}`,
  },
  method: "POST",
  body: JSON.stringify(requestData),
});

const output = await response.json();

   (BUTTON) Copy

Input schema

     * audio string
          + Input audio file as a [90]Data URL.
     * language string
          + Language of the input text. Whisper auto-detects the language
            if not provided. See the full list of options [91]here
          + Default: "
     * diarize_audio Boolean
          + Whether to diarize the audio.
          + Default: false
     * batch_size integer
          + Number of parallel batches.
          + Default: 24

Output schema

   This endpoint outputs a JSON with two fields:
     * text string
     * chunks Chunk[]

   Here, Chunk is a JSON object with the following fields:
     * speaker string
          + [Optional] only present if diarize_audio is true
     * text string
     * timestamp [float, float]

Stream text-to-speech with coqui-ai/TTS

   XTTS v2 is a fast and high-quality text-to-speech model.

   This endpoint uses a streaming version of [92]coqui-ai/TTS that streams
   wav audio back as it's generated in real-time.

Example code

// pages/api/modal.ts
const requestData = {
  text: "It is a mistake to think you can solve any major problems just with pot
atoes.",
  language: "en",
};
const result = await fetch("https://modal-labs--instant-xtts-v2.modal.run", {
  headers: {
    Authorization: `Token ${process.env.MODAL_TOKEN_ID}:${process.env.MODAL_TOKE
N_SECRET}`,
    "Content-Type": "application/json",
  },
  method: "POST",
  body: JSON.stringify(requestData),
});
const audioBuffer = await response.buffer();

   (BUTTON) Copy

Input schema

     * text string
          + Input text
     * language string
          + Language of the input text
          + One of: en, es, fr, de, it, pt, pl, tr, ru, nl, cs, ar, zh,
            hu, ko, hi
          + Default: en

Output schema

   This endpoint streams bytes for a single audio file with [93]media type
   "audio/wav".

Want more?

   If a popular open-source AI model API is not listed here, you can
   [94]either implement it in Python and host it on Modal or [95]ask us in
   Slack to add it as an Instant Endpoint!
   [96]Connecting Modal to your Vercel account [97]What this integration
   does [98]Authentication [99]Installing the integration
   [100]Uninstalling the integration [101]Modal Instant Endpoints
   [102]Stable Diffusion XL [103]Example code [104]Input schema
   [105]Output schema [106]Pricing [107]Transcribe speech with
   vaibhavs10/insanely-fast-whisper [108]Example code [109]Input schema
   [110]Output schema [111]Stream text-to-speech with coqui-ai/TTS
   [112]Example code [113]Input schema [114]Output schema [115]Want more?
   Modal logo Â© 2024
   [116]About [117]Status [118]Changelog [119]Documentation [120]Slack
   Community [121]Pricing [122]Examples

