Page: modal.com_files/modal.com/docs/examples/torch_profiling.html
----------------------------------------
   [1]Modal logo
   [2]Guide [3]Examples [4]Reference [5]Playground
   [6]Log In [7]Sign Up
   (BUTTON)
   [8]Featured[9]Getting started [10]Hello, world[11]Simple web
   scraper[12]Serving web endpoints[13]Large language models (LLMs)
   [14]Deploy an OpenAI-compatible LLM service with
   vLLM[15]High-throughput serverless TensorRT-LLM[16]Run Vision-Language
   Models with SGLang[17]Deploy a Moshi voice chatbot[18]Run a multimodal
   RAG chatbot to answer questions about PDFs[19]Fine-tune an LLM with
   Axolotl[20]Replace your CEO with an LLM[21]Diffusion models [22]Run
   Flux fast with torch.compile[23]Fine-tune an image generator on your
   pet[24]Generate video clips with Mochi[25]Transform images with Stable
   Diffusion XL Turbo[26]Deploy ControlNet demos with Gradio[27]Run a
   music-generating Discord bot[28]Training models from scratch [29]Train
   an SLM with early-stopping grid search over hyperparameters[30]Run
   long, resumable training jobs[31]Sandboxed code execution [32]Run a
   LangGraph agent's code in a secure GPU sandbox[33]Build a stateful,
   sandboxed code interpreter[34]Run Node.js, Ruby, and more in a
   Sandbox[35]Run a sandboxed Jupyter notebook[36]Parallel processing and
   job scheduling [37]Transcribe podcasts with Whisper[38]Deploy a Hacker
   News Slackbot[39]Run a Document OCR job queue[40]Serve a Document OCR
   web app[41]Hosting popular libraries [42]FastHTML: Deploy 100,000
   multiplayer checkboxes[43]YOLO: Fine-tuning and serve computer vision
   models[44]MultiOn: Create an agent for AI news[45]Blender: Build a 3D
   render farm[46]Streamlit: Run and deploy Streamlit apps[47]ComfyUI: Run
   ComfyUI interactively and as an API[48]SQLite: Publish explorable data
   with Datasette[49]Y! Finance: Process stock prices in
   parallel[50]Algolia: Build docsearch with a crawler[51]Connecting to
   other APIs [52]MongoDB: Vector and geospatial search over satellite
   images[53]Google Sheets: Sync databases and APIs to a Google
   Sheet[54]LangChain: Run a RAG Q&A chatbot[55]Tailscale: Add Modal Apps
   to your VPN[56]Prometheus: Publish custom metrics with
   Pushgateway[57]Managing data [58]Mount S3 buckets in Modal
   apps[59]Build your own data warehouse with DuckDB, DBT, and
   Modal[60]Create a LoRA Playground with Modal, Gradio, and
   S3[61]Miscellaneous
     __________________________________________________________________

   [62]View on GitHub

Tracing and profiling GPU-accelerated PyTorch programs on Modal

   A PyTorch trace loaded into ui.perfetto.dev

   GPUs are high-performance computing devices. For high-performance
   computing, tools for measuring and investigating performance are as
   critical as tools for testing and confirming correctness in typical
   software.

   In this example, we demonstrate how to wrap a Modal Function with
   PyTorch's built-in profiler, which captures events on both CPUs & GPUs.
   We also show how to host TensorBoard, which includes useful
   visualizations and performance improvement suggestions.

   For a live walkthrough, check out [63]this video on our YouTube
   channel.

Saving traces to a Modal Volume

   Most tracing tools, including PyTorch's profiler, produce results as
   files on disk. Modal Functions run in ephemeral containers in Modal's
   cloud infrastructure, so by default these files disappear as soon as
   the Function finishes running.

   We can ensure these files persist by saving them to a [64]Modal Volume.
   Volumes are a distributed file system: files can be read or written
   from by many machines across a network, in this case from inside any
   Modal Function.

   To start, we just create a Volume with a specific name. We'll also set
   a particular directory that we'll use for it in our Functions below,
   for convenience.
from pathlib import Path

import modal

traces = modal.Volume.from_name("example-traces", create_if_missing=True)
TRACE_DIR = Path("/traces")

   (BUTTON) Copy

Setting up a Modal App with a GPU-accelerated PyTorch Function

   We next set up the Modal Function that we wish to profile.

   In general, we want to attach profiling tools to code that's already in
   place and measure or debug its performance, and then detach it as
   easily as possible so that we can be confident that the same
   performance characteristics pertain in production.

   In keeping with that workflow, in this example we first define the
   Modal Function we want to profile, without including any of the
   profiling logic.

   That starts with the Function's environment: the Modal [65]App the
   Function is attached to, the container [66]Image with the Function's
   dependencies, and the hardware requirements of the Function, like a
   [67]GPU.
app = modal.App("example-torch-profiling")  # create an App

image = modal.Image.debian_slim(  # define dependencies
    python_version="3.11"
).pip_install("torch==2.5.1", "numpy==2.1.3")

with image.imports():  # set up common imports
    import torch

   (BUTTON) Copy

   Here, we define the config as a dictionary so that we can re-use it
   here and later, when we attach the profiler. We want to make sure the
   profiler is in the same environment!
config = {"gpu": "a10g", "image": image}

   (BUTTON) Copy

   The Function we target for profiling appears below. It's just some
   simple PyTorch logic that repeatedly multiplies a random matrix with
   itself.

   The logic is simple, but it demonstrates two common issues with
   GPU-accelerated Python code that are relatively easily fixed:

   We'll cover these in more detail once we have the profiler set up.
@app.function(**config)
def underutilize(scale=1):
    records = []

    x = torch.randn(  #  2: not enough work to keep the GPU busy
        scale * 100, scale * 100, device="cuda"
    )

    for ii in range(10):
        x = x @ x

        class Record:  #  1: heavy Python work in the hot loop
            def __init__(self, value):
                self.value = value

        records.append(Record(ii))

    x[0][0].cpu()  # force a host sync for accurate timing

   (BUTTON) Copy

Wrapping a Modal Function with a profiler

   Now, let's wrap our underutilize Function with another Modal Function
   that runs PyTorch's profiler while executing it.

   This Function has the same environment config as underutilize, but it
   also attaches a remote Modal Volume to save profiler outputs.

   To increase the flexibility of this approach, we allow it to take the
   target Function's name as an argument. That's not much use here where
   there's only one Function, but it makes it easier to copy-paste this
   code into your projects to add profiling.
@app.function(volumes={TRACE_DIR: traces}, **config)
def profile(
    function,
    label: str = None,
    steps: int = 3,
    schedule=None,
    record_shapes: bool = False,
    profile_memory: bool = False,
    with_stack: bool = False,
    print_rows: int = 0,
    **kwargs,
):
    from uuid import uuid4

    if isinstance(function, str):
        try:
            function = app.registered_functions[function]
        except KeyError:
            raise ValueError(f"Function {function} not found")
    function_name = function.tag

    output_dir = (
        TRACE_DIR
        / (function_name + (f"_{label}" if label else ""))
        / str(uuid4())
    )
    output_dir.mkdir(parents=True, exist_ok=True)

    if schedule is None:
        if steps < 3:
            raise ValueError(
                "Steps must be at least 3 when using default schedule"
            )
        schedule = {"wait": 1, "warmup": 1, "active": steps - 2, "repeat": 0}

    schedule = torch.profiler.schedule(**schedule)

    with torch.profiler.profile(
        activities=[
            torch.profiler.ProfilerActivity.CPU,
            torch.profiler.ProfilerActivity.CUDA,
        ],
        schedule=schedule,
        record_shapes=record_shapes,
        profile_memory=profile_memory,
        with_stack=with_stack,
        on_trace_ready=torch.profiler.tensorboard_trace_handler(output_dir),
    ) as prof:
        for _ in range(steps):
            function.local(**kwargs)  # <-- here we wrap the target Function
            prof.step()

    if print_rows:
        print(
            prof.key_averages().table(
                sort_by="cuda_time_total", row_limit=print_rows
            )
        )

    trace_path = sorted(
        output_dir.glob("**/*.pt.trace.json"),
        key=lambda pth: pth.stat().st_mtime,
        reverse=True,
    )[0]

    print(f"trace saved to {trace_path.relative_to(TRACE_DIR)}")

    return trace_path.read_text(), trace_path.relative_to(TRACE_DIR)

   (BUTTON) Copy

Triggering profiled execution from the command line and viewing in Perfetto

   We wrap one more layer to make this executable from the command line: a
   local_entrypoint that runs
modal run torch_profiling.py --function underutilize --print-rows 10

   (BUTTON) Copy
@app.local_entrypoint()
def main(
    function: str = "underutilize",
    label: str = None,
    steps: int = 3,
    schedule=None,
    record_shapes: bool = False,
    profile_memory: bool = False,
    with_stack: bool = False,
    print_rows: int = 10,
    kwargs_json_path: str = None,
):
    if kwargs_json_path is not None:  # use to pass arguments to function
        import json

        kwargs = json.loads(Path(kwargs_json_path).read_text())
    else:
        kwargs = {}

    results, remote_path = profile.remote(
        function,
        label=label,
        steps=steps,
        schedule=schedule,
        record_shapes=record_shapes,
        profile_memory=profile_memory,
        with_stack=with_stack,
        print_rows=print_rows,
        **kwargs,
    )

    output_path = Path("/tmp") / remote_path.name
    output_path.write_text(results)
    print(f"trace saved locally at {output_path}")

   (BUTTON) Copy

   Underneath the profile results, you'll also see the path at which the
   trace was saved on the Volume and the path at which it was saved
   locally.

   You can view the trace in the free online [68]Perfetto UI.

Improving the performance of our dummy test code

   The underutilize demonstrates two common patterns that leads to
   unnecessarily low GPU utilization:

   We simulated 1 in underutilize by defining a Python class in the middle
   of the matrix multiplication loop. This takes on the order of 10
   microseconds, roughly the same time it takes our A10 GPU to do the
   matrix multiplication. Move it out of the loop to observe a small
   improvement in utilization. In a real setting, this code might be
   useful logging or data processing logic, which we must carefully keep
   out of the way of the code driving work on the GPU.

   We simulated 2 in underutilize by providing a matrix that is too small
   to occupy the GPU for long. Increase the size of the matrix by a factor
   of 4 in each dimension (a factor of 16 total), to increase the
   utilization without increasing the execution time.

   This is an untuitive feature of GPU programming in general: much work
   is done concurrently and bottlenecks are non-obvious, so sometimes more
   work can be done for free or on the cheap. In a server for large
   generative models, this might mean producing multiple outputs per user
   or handling multiple users at the same time is more economical than it
   at first seems!

Serving TensorBoard on Modal to view PyTorch profiles and traces

   The TensorBoard experiment monitoring server also includes a plugin for
   viewing and interpreting the results of PyTorch profiler runs: the
   torch_tb_profiler plugin.
tb_image = modal.Image.debian_slim(python_version="3.11").pip_install(
    "tensorboard==2.18.0", "torch_tb_profiler==0.4.3"
)

   (BUTTON) Copy

   Because TensorBoard is a WSGI app, we can [69]host it on Modal with the
   modal.wsgi_app decorator.

   Making this work with Modal requires one extra step: we add some
   [70]WSGI Middleware that checks the Modal Volume for updates whenever
   the whole page is reloaded.
class VolumeMiddleware:
    def __init__(self, app):
        self.app = app

    def __call__(self, environ, start_response):
        if (route := environ.get("PATH_INFO")) in ["/", "/modal-volume-reload"]:
            try:
                traces.reload()
            except Exception as e:
                print("Exception while re-loading traces: ", e)
            if route == "/modal-volume-reload":
                environ["PATH_INFO"] = "/"  # redirect
        return self.app(environ, start_response)

   (BUTTON) Copy

   You can deploy the TensorBoard server defined below with the following
   command:
modal deploy torch_profiling

   (BUTTON) Copy

   and you can find your server at the URL printed to the terminal.
@app.function(
    volumes={TRACE_DIR: traces},
    image=tb_image,
    concurrency_limit=1,  # single replica
    allow_concurrent_inputs=100,  # 100 concurrent request threads
    container_idle_timeout=5 * 60,  # five minute idle time
)
@modal.wsgi_app()
def tensorboard():
    import tensorboard

    board = tensorboard.program.TensorBoard()
    board.configure(logdir=str(TRACE_DIR))
    (data_provider, deprecated_multiplexer) = board._make_data_provider()
    wsgi_app = tensorboard.backend.application.TensorBoardWSGIApp(
        board.flags,
        board.plugin_loaders,
        data_provider,
        board.assets_zip_provider,
        deprecated_multiplexer,
        experimental_middlewares=[VolumeMiddleware],
    )

    return wsgi_app._create_wsgi_app()

   (BUTTON) Copy
   [71]Tracing and profiling GPU-accelerated PyTorch programs on Modal
   [72]Saving traces to a Modal Volume [73]Setting up a Modal App with a
   GPU-accelerated PyTorch Function [74]Wrapping a Modal Function with a
   profiler [75]Triggering profiled execution from the command line and
   viewing in Perfetto [76]Improving the performance of our dummy test
   code [77]Serving TensorBoard on Modal to view PyTorch profiles and
   traces

Try this on Modal!

   You can run this example on Modal in 60 seconds.
   [78]Create account to run

   After creating a free account, install the Modal Python package, and
   create an API token.
   $
pip install modal

   $
modal setup

   (BUTTON) Copy

   Clone the [79]modal-examples repository and run:
   $
git clone https://github.com/modal-labs/modal-examples

   $
cd modal-examples

   $
modal run 06_gpu_and_ml/torch_profiling.py

   (BUTTON) Copy
   Modal logo © 2024
   [80]About [81]Status [82]Changelog [83]Documentation [84]Slack
   Community [85]Pricing [86]Examples

