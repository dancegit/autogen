Page: modal.com_files/modal.com/docs/examples/agent.html
----------------------------------------
   [1]Modal logo
   [2]Guide [3]Examples [4]Reference [5]Playground
   [6]Log In [7]Sign Up
   (BUTTON)
   [8]Featured[9]Getting started [10]Hello, world[11]Simple web
   scraper[12]Serving web endpoints[13]Large language models (LLMs)
   [14]Deploy an OpenAI-compatible LLM service with
   vLLM[15]High-throughput serverless TensorRT-LLM[16]Run Vision-Language
   Models with SGLang[17]Deploy a Moshi voice chatbot[18]Run a multimodal
   RAG chatbot to answer questions about PDFs[19]Fine-tune an LLM with
   Axolotl[20]Replace your CEO with an LLM[21]Diffusion models [22]Run
   Flux fast with torch.compile[23]Fine-tune an image generator on your
   pet[24]Generate video clips with Mochi[25]Transform images with Stable
   Diffusion XL Turbo[26]Deploy ControlNet demos with Gradio[27]Run a
   music-generating Discord bot[28]Training models from scratch [29]Train
   an SLM with early-stopping grid search over hyperparameters[30]Run
   long, resumable training jobs[31]Sandboxed code execution [32]Run a
   LangGraph agent's code in a secure GPU sandbox[33]Build a stateful,
   sandboxed code interpreter[34]Run Node.js, Ruby, and more in a
   Sandbox[35]Run a sandboxed Jupyter notebook[36]Parallel processing and
   job scheduling [37]Transcribe podcasts with Whisper[38]Deploy a Hacker
   News Slackbot[39]Run a Document OCR job queue[40]Serve a Document OCR
   web app[41]Hosting popular libraries [42]FastHTML: Deploy 100,000
   multiplayer checkboxes[43]YOLO: Fine-tuning and serve computer vision
   models[44]MultiOn: Create an agent for AI news[45]Blender: Build a 3D
   render farm[46]Streamlit: Run and deploy Streamlit apps[47]ComfyUI: Run
   ComfyUI interactively and as an API[48]SQLite: Publish explorable data
   with Datasette[49]Y! Finance: Process stock prices in
   parallel[50]Algolia: Build docsearch with a crawler[51]Connecting to
   other APIs [52]MongoDB: Vector and geospatial search over satellite
   images[53]Google Sheets: Sync databases and APIs to a Google
   Sheet[54]LangChain: Run a RAG Q&A chatbot[55]Tailscale: Add Modal Apps
   to your VPN[56]Prometheus: Publish custom metrics with
   Pushgateway[57]Managing data [58]Mount S3 buckets in Modal
   apps[59]Build your own data warehouse with DuckDB, DBT, and
   Modal[60]Create a LoRA Playground with Modal, Gradio, and
   S3[61]Miscellaneous
     __________________________________________________________________

   [62]View on GitHub

Build a coding agent with Modal Sandboxes and LangGraph

   This example demonstrates how to build an LLM coding "agent" that can
   generate and execute Python code, using documentation from the web to
   inform its approach.

   Naturally, we use the agent to generate code that runs language models.

   The agent is built with [63]LangGraph, a library for building directed
   graphs of computation popular with AI agent developers, and uses models
   from the OpenAI API.

Setup

import modal

from .src import edges, nodes, retrieval
from .src.common import COLOR, PYTHON_VERSION, image

   (BUTTON) Copy

   You will need two [64]Modal Secrets to run this example: one to access
   the OpenAI API and another to access the LangSmith API for logging the
   agent's behavior.

   To create them, head to the [65]Secrets dashboard, select "Create new
   secret", and use the provided templates for OpenAI and LangSmith.
app = modal.App(
    "example-code-langchain",
    image=image,
    secrets=[
        modal.Secret.from_name("openai-secret"),
        modal.Secret.from_name("my-langsmith-secret"),
    ],
)

   (BUTTON) Copy

Creating a Sandbox

   We execute the agent's code in a Modal [66]Sandbox, which allows us to
   run arbitrary code in a safe environment. In this example, we will use
   the [67]transformers library to generate text with a pre-trained model.
   Let's create a Sandbox with the necessary dependencies.
def create_sandbox(app) -> modal.Sandbox:
    # Change this image (and the retrieval logic in the retrieval module)
    # if you want the agent to give coding advice on other libraries!
    agent_image = modal.Image.debian_slim(
        python_version=PYTHON_VERSION
    ).pip_install(
        "torch==2.5.0",
        "transformers==4.46.0",
    )

    return modal.Sandbox.create(
        image=agent_image,
        timeout=60 * 10,  # 10 minutes
        app=app,
        # Modal sandboxes support GPUs!
        gpu="T4",
        # you can also pass secrets here -- note that the main app's secrets are
 not shared
    )

   (BUTTON) Copy

   We also need a way to run our code in the sandbox. For this, we'll
   write a simple wrapper around the Modal Sandox exec method. We use exec
   because it allows us to run code without spinning up a new container.
   And we can reuse the same container for multiple runs, preserving
   state.
def run(code: str, sb: modal.Sandbox) -> tuple[str, str]:
    print(
        f"{COLOR['HEADER']}¦: Running in sandbox{COLOR['ENDC']}",
        f"{COLOR['GREEN']}{code}{COLOR['ENDC']}",
        sep="\n",
    )

    exc = sb.exec("python", "-c", code)
    exc.wait()

    stdout = exc.stdout.read()
    stderr = exc.stderr.read()

    if exc.returncode != 0:
        print(
            f"{COLOR['HEADER']}¦: Failed with exitcode {sb.returncode}{COLOR['EN
DC']}"
        )

    return stdout, stderr

   (BUTTON) Copy

Constructing the agent's graph

   Now that we have the sandbox to execute code in, we can construct our
   agent's graph. Our graph is defined in the edges and nodes modules
   [68]associated with this example. Nodes are actions that change the
   state. Edges are transitions between nodes.

   The idea is simple: we start at the node generate, which invokes the
   LLM to generate code based off documentation. The generated code is
   executed (in the sandbox) as part of an edge called
   check_code_execution and then the outputs are passed to the LLM for
   evaluation (the evaluate_execution node). If the LLM determines that
   the code has executed correctly -- which might mean that the code
   raised an exception! -- we pass along the decide_to_finish edge and
   finish.
def construct_graph(sandbox: modal.Sandbox, debug: bool = False):
    from langgraph.graph import StateGraph

    from .src.common import GraphState

    # Crawl the transformers documentation to inform our code generation
    context = retrieval.retrieve_docs(debug=debug)

    graph = StateGraph(GraphState)

    # Attach our nodes to the graph
    graph_nodes = nodes.Nodes(context, sandbox, run, debug=debug)
    for key, value in graph_nodes.node_map.items():
        graph.add_node(key, value)

    # Construct the graph by adding edges
    graph = edges.enrich(graph)

    # Set the starting and ending nodes of the graph
    graph.set_entry_point(key="generate")
    graph.set_finish_point(key="finish")

    return graph

   (BUTTON) Copy

   We now set up the graph and compile it. See the src module for details
   on the content of the graph and the nodes we've defined.
DEFAULT_QUESTION = "How do I generate Python code using a pre-trained model from
 the transformers library?"


@app.function()
def go(
    question: str = DEFAULT_QUESTION,
    debug: bool = False,
):
    """Compiles the Python code generation agent graph and runs it, returning th
e result."""
    sb = create_sandbox(app)

    graph = construct_graph(sb, debug=debug)
    runnable = graph.compile()
    result = runnable.invoke(
        {"keys": {"question": question, "iterations": 0}},
        config={"recursion_limit": 50},
    )

    sb.terminate()

    return result["keys"]["response"]

   (BUTTON) Copy

Running the Graph

   Now let's call the agent from the command line!

   We define a local_entrypoint that runs locally and triggers execution
   on Modal.

   You can invoke it by executing following command from a folder that
   contains the codelangchain directory [69]from our examples repo:
modal run codelangchain.agent --question "How do I run a pre-trained model from
the transformers library?"

   (BUTTON) Copy
@app.local_entrypoint()
def main(
    question: str = DEFAULT_QUESTION,
    debug: bool = False,
):
    """Sends a question to the Python code generation agent.

    Switch to debug mode for shorter context and smaller model."""
    if debug:
        if question == DEFAULT_QUESTION:
            question = "hi there, how are you?"

    print(go.remote(question, debug=debug))

   (BUTTON) Copy

   If things are working properly, you should see output like the
   following:
$ modal run agent.py --question "generate some cool output with transformers"
---DECISION: FINISH---
---FINISHING---
To generate some cool output using transformers, we can use a pre-trained langua
ge model from the Hugging Face Transformers library. In this example, we'll use
the GPT-2 model to generate text based on a given prompt. The GPT-2 model is a p
opular choice for text generation tasks due to its ability to produce coherent a
nd contextually relevant text. We'll use the pipeline API from the Transformers
library, which simplifies the process of using pre-trained models for various ta
sks, including text generation.

from transformers import pipeline
# Initialize the text generation pipeline with the GPT-2 model
generator = pipeline('text-generation', model='gpt2')

# Define a prompt for the model to generate text from
prompt = "Once upon a time in a land far, far away"

# Generate text using the model
output = generator(prompt, max_length=50, num_return_sequences=1)

# Print the generated text
print(output[0]['generated_text'])

Result of code execution:
Once upon a time in a land far, far away, and still inhabited even after all the
 human race, there would be one God: a perfect universal God who has always been
 and will ever be worshipped. All His acts and deeds are immutable,

   (BUTTON) Copy
   [70]Build a coding agent with Modal Sandboxes and LangGraph [71]Setup
   [72]Creating a Sandbox [73]Constructing the agent's graph [74]Running
   the Graph

Try this on Modal!

   You can run this example on Modal in 60 seconds.
   [75]Create account to run

   After creating a free account, install the Modal Python package, and
   create an API token.
   $
pip install modal

   $
modal setup

   (BUTTON) Copy

   Clone the [76]modal-examples repository and run:
   $
git clone https://github.com/modal-labs/modal-examples

   $
cd modal-examples

   $
modal run 13_sandboxes.codelangchain.agent --question 'Use gpt2 and transformers
 to generate text'

   (BUTTON) Copy
   Modal logo © 2024
   [77]About [78]Status [79]Changelog [80]Documentation [81]Slack
   Community [82]Pricing [83]Examples

