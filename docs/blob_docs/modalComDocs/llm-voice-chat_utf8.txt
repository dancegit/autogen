Page: modal.com_files/modal.com/docs/examples/llm-voice-chat.html
----------------------------------------
   [1]Modal logo
   [2]Guide [3]Examples [4]Reference [5]Playground
   [6]Log In [7]Sign Up
   (BUTTON)
   [8]Featured[9]Getting started [10]Hello, world[11]Simple web
   scraper[12]Serving web endpoints[13]Large language models (LLMs)
   [14]Deploy an OpenAI-compatible LLM service with
   vLLM[15]High-throughput serverless TensorRT-LLM[16]Run Vision-Language
   Models with SGLang[17]Deploy a Moshi voice chatbot[18]Run a multimodal
   RAG chatbot to answer questions about PDFs[19]Fine-tune an LLM with
   Axolotl[20]Replace your CEO with an LLM[21]Diffusion models [22]Run
   Flux fast with torch.compile[23]Fine-tune an image generator on your
   pet[24]Generate video clips with Mochi[25]Transform images with Stable
   Diffusion XL Turbo[26]Deploy ControlNet demos with Gradio[27]Run a
   music-generating Discord bot[28]Training models from scratch [29]Train
   an SLM with early-stopping grid search over hyperparameters[30]Run
   long, resumable training jobs[31]Sandboxed code execution [32]Run a
   LangGraph agent's code in a secure GPU sandbox[33]Build a stateful,
   sandboxed code interpreter[34]Run Node.js, Ruby, and more in a
   Sandbox[35]Run a sandboxed Jupyter notebook[36]Parallel processing and
   job scheduling [37]Transcribe podcasts with Whisper[38]Deploy a Hacker
   News Slackbot[39]Run a Document OCR job queue[40]Serve a Document OCR
   web app[41]Hosting popular libraries [42]FastHTML: Deploy 100,000
   multiplayer checkboxes[43]YOLO: Fine-tuning and serve computer vision
   models[44]MultiOn: Create an agent for AI news[45]Blender: Build a 3D
   render farm[46]Streamlit: Run and deploy Streamlit apps[47]ComfyUI: Run
   ComfyUI interactively and as an API[48]SQLite: Publish explorable data
   with Datasette[49]Y! Finance: Process stock prices in
   parallel[50]Algolia: Build docsearch with a crawler[51]Connecting to
   other APIs [52]MongoDB: Vector and geospatial search over satellite
   images[53]Google Sheets: Sync databases and APIs to a Google
   Sheet[54]LangChain: Run a RAG Q&A chatbot[55]Tailscale: Add Modal Apps
   to your VPN[56]Prometheus: Publish custom metrics with
   Pushgateway[57]Managing data [58]Mount S3 buckets in Modal
   apps[59]Build your own data warehouse with DuckDB, DBT, and
   Modal[60]Create a LoRA Playground with Modal, Gradio, and
   S3[61]Miscellaneous
     __________________________________________________________________

QuiLLMan: Voice Chat with Moshi

   [62]QuiLLMan is a complete voice chat application built on Modal: you
   speak and the chatbot speaks back!

   At the core is Kyutai Lab's [63]Moshi model, a speech-to-speech
   language model that will continuously listen, plan, and respond to the
   user.

   Thanks to bidirectional websocket streaming and [64]Opus audio
   compression, response times on good internet can be nearly
   instantaneous, closely matching the cadence of human speech.

   You can find the demo live [65]here.

   Quillman

   Everything -- from the React frontend to the model backend -- is
   deployed serverlessly on Modal, allowing it to automatically scale and
   ensuring you only pay for the compute you use.

   This page provides a high-level walkthrough of the [66]GitHub repo.

Code overview

   Traditionally, building a bidirectional streaming web application as
   compute-heavy as QuiLLMan would take a lot of work, and it's especially
   difficult to make it robust and scale to handle many concurrent users.

   But with Modal, it's as simple as writing two different classes and
   running a CLI command.

   Our project structure looks like this:
       and maintains a bidirectional websocket connection with the client.

   Let's go through each of these components in more detail.

FastAPI Server

   Both frontend and backend are served via a [69]FastAPI Server, which is
   a popular Python web framework for building REST APIs.

   On Modal, a function or class method can be exposed as a web endpoint
   by decorating it with [70]@app.asgi_app() and returning a FastAPI app.
   You're then free to configure the FastAPI server however you like,
   including adding middleware, serving static files, and running
   websockets.

Moshi Websocket Server

   Traditionally, a speech-to-speech chat app requires three distinct
   modules: speech-to-text, text-to-text, and text-to-speech. Passing data
   between these modules introduces bottlenecks, and can limit the speed
   of the app and forces a turn-by-turn conversation which can feel
   unnatural.

   Kyutai Lab's [71]Moshi bundles all modalities into one model, which
   decreases latency and makes for a much simpler app.

   Under the hood, Moshi uses the [72]Mimi streaming encoder/decoder model
   to maintain an unbroken stream of audio in and out. The encoded audio
   is processed by a [73]speech-text foundation model, which uses an
   internal monologue to determine when and how to respond.

   Using a streaming model introduces a few challenges not normally seen
   in inference backends:
       conversation so far. This means a model instance cannot be shared
       between user conversations, so we must run a unique GPU per user
       session, which is normally not an easy feat!
       as a POST request. We must find a way to stream audio data in and
       out, and do it fast enough for seamless playback.

   We solve both of these in src/moshi.py, using a few Modal features.

   To solve statefulness, we just spin up a new GPU per concurrent user.
   That's easy with Modal!
@app.cls(
    image=image,
    gpu="A10G",
    container_idle_timeout=300,
    ...
)
class Moshi:
    # ...

   (BUTTON) Copy

   With this setting, if a new user connects, a new GPU instance is
   created! When any user disconnects, the state of their model is reset
   and that GPU instance is returned to the warm pool for re-use (for up
   to 300 seconds). Be aware that a GPU per user is not going to be cheap,
   but it's the simplest way to ensure user sessions are isolated.

   For streaming, we use FastAPI's support for bidirectional websockets.
   This allows clients to establish a single connection at the start of
   their session, and stream audio data both ways.

   Just as a FastAPI server can run from a Modal function, it can also be
   attached to a Modal class method, allowing us to couple a prewarmed
   Moshi model to a websocket session.
@modal.asgi_app()
def web(self):
    from fastapi import FastAPI, Response, WebSocket, WebSocketDisconnect

    web_app = FastAPI()
    @web_app.websocket("/ws")
    async def websocket(ws: WebSocket):
        with torch.no_grad():
            await ws.accept()

            # handle user session

            # spawn loops for async IO
            async def recv_loop():
                while True:
                    data = await ws.receive_bytes()
                    # send data into inference stream...

            async def send_loop():
                while True:
                    await asyncio.sleep(0.001)
                    msg = self.opus_stream_outbound.read_bytes()
                    # send inference output to user ...

   (BUTTON) Copy

   To run a [74]development server for the Moshi module, run this command
   from the root of the repo.
modal serve src.moshi

   (BUTTON) Copy

   In the terminal output, you'll find a URL for creating a websocket
   connection.

React Frontend

   The frontend is a static React app, found in the src/frontend directory
   and served by src/app.py.

   We use the [75]Web Audio API to record audio from the user's microphone
   and playback audio responses from the model.

   For efficient audio transmission, we use the [76]Opus codec to compress
   audio across the network. Opus recording and playback are supported by
   the [77]opus-recorder and [78]ogg-opus-decoder libraries.

   To serve the frontend assets, run this command from the root of the
   repo.
modal serve src.app

   (BUTTON) Copy

   Since src/app.py imports the src/moshi.py module, this serve command
   also serves the Moshi websocket server as its own endpoint.

Deploy

   When you're ready to go live, use the deploy command to deploy the app
   to Modal.
modal deploy src.app

   (BUTTON) Copy

Steal this example

   The code for this entire example is [79]available on GitHub, so feel
   free to fork it and make it your own!
   [80]QuiLLMan: Voice Chat with Moshi [81]Code overview [82]FastAPI
   Server [83]Moshi Websocket Server [84]React Frontend [85]Deploy
   [86]Steal this example
   Modal logo Â© 2024
   [87]About [88]Status [89]Changelog [90]Documentation [91]Slack
   Community [92]Pricing [93]Examples

