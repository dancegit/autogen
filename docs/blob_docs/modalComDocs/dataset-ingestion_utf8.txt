Page: modal.com_files/modal.com/docs/guide/dataset-ingestion.html
----------------------------------------
   [1]Modal logo
   [2]Guide [3]Examples [4]Reference [5]Playground
   [6]Log In [7]Sign Up
   (BUTTON)
   [8]Introduction[9]Custom container images [10]Custom
   containers[11]Private registries[12]GPUs and other resources [13]GPU
   acceleration[14]Using CUDA on Modal[15]Reserving CPU and
   memory[16]Scaling out [17]Scaling out[18]Dicts and queues[19]Job
   processing[20]Concurrent inputs on a single container (beta)[21]Dynamic
   batching (beta)[22]Scheduling and cron jobs[23]Deployment [24]Apps,
   Stubs, and entrypoints[25]Managing deployments[26]Invoke deployed
   functions[27]Continuous deployment[28]Secrets and environment variables
   [29]Secrets[30]Environment variables[31]Web endpoints [32]Web
   endpoints[33]Streaming endpoints[34]Web endpoint URLs[35]Request
   timeouts[36]Networking [37]Tunnels (beta)[38]Proxies (beta)[39]Data
   sharing and storage [40]Passing local data[41]Volumes[42]Mounting local
   files and directories[43]Storing model weights[44]Dataset
   ingestion[45]Cloud bucket mounts[46]Network file systems
   (superseded)[47]Sandboxes [48]Sandboxes[49]Running
   commands[50]Networking and security[51]File access[52]Performance
   [53]Cold start performance[54]Memory Snapshot (beta)[55]Geographic
   latency[56]Reliability and robustness [57]Failures and
   retries[58]Preemption[59]Timeouts[60]Troubleshooting[61]Security and
   privacy[62]Integrations [63]Connecting Modal to your Vercel
   account[64]Connecting Modal to your Datadog account[65]Connecting Modal
   to your OpenTelemetry provider[66]Okta SSO[67]Slack notifications
   (beta)[68]Other topics [69]File and project structure[70]Developing and
   debugging[71]Modal user account
   setup[72]Workspaces[73]Environments[74]Jupyter
   notebooks[75]Asynchronous API usage[76]Global variables[77]Region
   selection[78]Container lifecycle hooks[79]Parameterized functions[80]S3
   Gateway endpoints
     __________________________________________________________________

Large dataset ingestion

   This guide provides best practices for downloading, transforming, and
   storing large datasets within Modal. A dataset is considered large if
   it contains hundreds of thousands of files and/or is over 100 GiB in
   size.

   These guidelines ensure that large datasets can be ingested fully and
   reliably.

Configure your Function for heavy disk usage

   Large datasets should be downloaded and transformed using a
   modal.Function and stored into a modal.CloudBucketMount. We recommend
   backing the latter with a Cloudflare R2 bucket, because Cloudflare does
   not charge network egress fees and has lower GiB/month storage costs
   than AWS S3.

   This modal.Function should specify a large timeout because large
   dataset processing can take hours, and it should request a larger
   ephemeral disk in cases where the dataset being downloaded and
   processed is hundreds of GiBs.
@app.function(
    volumes={
        "/mnt": modal.CloudBucketMount(
            "datasets",
            bucket_endpoint_url="https://abc123example.r2.cloudflarestorage.com"
,
            secret=modal.Secret.from_name("cloudflare-r2-datasets"),
        )
    },
    ephemeral_disk=1000 * 1000,  # 1 TiB
    timeout=60 * 60 * 12,  # 12 hours

)
def download_and_transform() -> None:
    ...

   (BUTTON) Copy

Use compressed archives on Modal Volumes

   modal.Volumes are designed for storing tens of thousands of individual
   files, but not for hundreds of thousands or millions of files. However
   they can be still be used for storing large datasets if files are first
   combined and compressed in a dataset transformation step before saving
   them into the Volume.

   See the [81]transforming section below for more details.

Experimentation

   Downloading and transforming large datasets can be fiddly. While
   iterating on a reliable ingestion program it is recommended to start a
   long-running modal.Function serving a JupyterHub server so that you can
   maintain disk state in the face of application errors.

   See the [82]running Jupyter server within a Modal function example as
   base code.

Downloading

   The raw dataset data should be first downloaded into the container at
   /tmp/ and not placed directly into the mounted volume. This serves a
   couple purposes.
       operations not supported properly by CloudBucketMount.
       which case it is wasteful to store it permanently.

   This snippet shows the basic download-and-copy procedure:
import pathlib
import shutil
import subprocess

tmp_path = pathlib.Path("/tmp/imagenet/")
vol_path = pathlib.Path("/mnt/imagenet/")
filename = "imagenet-object-localization-challenge.zip"
# 1. Download into /tmp/
subprocess.run(
    f"kaggle competitions download -c imagenet-object-localization-challenge --p
ath {tmp_path}",
    shell=True,
    check=True
)
vol_path.mkdir(exist_ok=True)
# 2. Copy (without transform) into mounted volume.
shutil.copyfile(tmp_path / filename, vol_path / filename)

   (BUTTON) Copy

Transforming

   When ingesting a large dataset it is sometimes necessary to tranform it
   before storage, so that it is in an optimal format for loading at
   runtime. A common kind of necessary transform is gzip decompression.
   Very large datasets are often gzipped for storage and network
   transmission efficiency, but gzip decompression (80 MiB/s) is hundreds
   of times slower than reading from a solid state drive (SSD) and should
   be done once before storage to avoid decompressing on every read
   against the dataset.

   Transformations should be performed after storing the raw dataset in
   /tmp/. Performing transformations almost always increases container
   disk usage and this is where the [83]ephemeral_disk parameter parameter
   becomes important. For example, a 100 GiB raw, compressed dataset may
   decompress to into 500 GiB, occupying 600 GiB of container disk space.

   Transformations should also typically be performed against /tmp/. This
   is because
       local SSD.
       permanently.

Examples

   The best practices offered in this guide are demonstrated in the
   [84]modal-examples repository.

   The examples include these popular large datasets:
     * [85]ImageNet, the image labeling dataset that kicked off the deep
       learning revolution
     * [86]COCO, the Common Objects in COntext dataset of densely-labeled
       images
     * [87]LAION-400M, the Stable Diffusion training dataset
     * Data derived from the [88]Big "Fantastic" Database, [89]Protein
       Data Bank, and [90]UniProt Database used in training the
       [91]RoseTTAFold protein structure model

   [92]Large dataset ingestion [93]Configure your Function for heavy disk
   usage [94]Use compressed archives on Modal Volumes [95]Experimentation
   [96]Downloading [97]Transforming [98]Examples
   Modal logo Â© 2024
   [99]About [100]Status [101]Changelog [102]Documentation [103]Slack
   Community [104]Pricing [105]Examples

