Page: aider.chat_files/aider.chat/2024/06/02/main-swe-bench.html
----------------------------------------
   #[1]RSS Feed

   [2]Skip to main content

   [3]aider (BUTTON)

     * [4]Home
     * (BUTTON) [5]Installation
          + [6]Installing aider
          + [7]Optional steps
          + [8]Aider with docker
          + [9]Install with pipx
          + [10]GitHub Codespaces
     * (BUTTON) [11]Usage
          + [12]Tips
          + [13]In-chat commands
          + [14]Chat modes
          + [15]Tutorial videos
          + [16]Voice-to-code with aider
          + [17]Images & web pages
          + [18]Prompt caching
          + [19]Aider in your browser
          + [20]Specifying coding conventions
          + [21]Linting and testing
     * (BUTTON) [22]Connecting to LLMs
          + [23]OpenAI
          + [24]Anthropic
          + [25]Gemini
          + [26]GROQ
          + [27]Azure
          + [28]Cohere
          + [29]DeepSeek
          + [30]Ollama
          + [31]OpenAI compatible APIs
          + [32]OpenRouter
          + [33]Vertex AI
          + [34]Amazon Bedrock
          + [35]Other LLMs
          + [36]Editing format
          + [37]Model warnings
     * (BUTTON) [38]Configuration
          + [39]Options reference
          + [40]YAML config file
          + [41]Config with .env
          + [42]Advanced model settings
     * (BUTTON) [43]Troubleshooting
          + [44]File editing problems
          + [45]Model warnings
          + [46]Token limits
          + [47]Aider not found
          + [48]Dependency versions
          + [49]Using /help
     * (BUTTON) [50]Example chat transcripts
          + [51]Create a simple flask app with aider
          + [52]Modify an open source 2048 game with aider
          + [53]A complex multi-file change, with debugging
          + [54]Create a "black box" test case
          + [55]Automatically update docs with aider
          + [56]Build pong with aider and pygame.
          + [57]Complete a css exercise with aider
          + [58]Download, analyze and plot US Census data
          + [59]Editing an asciinema cast file with aider
          + [60]Hello aider!
          + [61]Honor the NO_COLOR environment variable
          + [62]Improve css styling of chat transcripts
          + [63]Semantic search & replace code with aider
     * (BUTTON) [64]More info
          + [65]Git integration
          + [66]Supported languages
          + [67]Repository map
          + [68]Scripting aider
          + [69]Infinite output
          + [70]Edit formats
          + [71]Analytics
          + [72]Privacy policy
          + [73]Release history
     * [74]FAQ
     * [75]Aider LLM Leaderboards
     * [76]Aider blog

     * [77]GitHub
     * [78]Discord

   Aider is AI pair programming in your terminal. Aider is on [79]GitHub
   and [80]Discord.

   ____________________

     * [81]GitHub
     * [82]Discord
     * [83]Blog

   June 02, 2024

Aider is SOTA for both SWE Bench and SWE Bench Lite

   Aider scored 18.9% on the main [84]SWE Bench benchmark, achieving a
   state-of-the-art result. The current top leaderboard entry is 13.8%
   from Amazon Q Developer Agent. The best result reported elsewhere seems
   to be [85]13.9% from Devin.

   This result on the main SWE Bench builds on [86]aider's recent SOTA
   result on the easier SWE Bench Lite.

   [87]SWE Bench results

   All of aider's results reported here are pass@1 results, obtained
   without using the SWE Bench hints_text. Aider was benchmarked on the
   same [88]570 randomly selected SWE Bench problems that were used in the
   [89]Devin evaluation. See the [90]references for more details on the
   data presented in this chart.

Interactive, not agentic

   Aider achieved this result mainly through its existing features that
   focus on static code analysis, reliable LLM code editing, and pragmatic
   UX for automatically fixing linting and testing errors. Aider
   intentionally has quite limited and narrow "agentic behavior" to avoid
   long delays, high token costs and the need for users to repeatedly code
   review incorrect solutions. It's also worth noting that aider currently
   does not use RAG, vector search, tools or give the LLM access to search
   the web or unilaterally execute code.

   Aider is first and foremost an interactive tool for engineers to get
   real work done in real code bases using a chat interface. Aider
   provides a pair programming UX where users can ask for a change and see
   code edits performed in real-time. Aider can also offer additional help
   like fixing lint or test errors, but the user is always in full
   interactive control. This allows them to quickly steer
   misunderstandings back on course and avoid wasting time and token
   costs.

Benchmark methodology

   Benchmarking was conducted as follows:
     * Aider with GPT-4o was launched in each problem's git repository
       with the problem statement submitted as the opening chat message
       from "the user".
     * After that aider ran as normal, except all of aider's suggestions
       were always accepted without user approval.
     * A [91]simple harness was used to retry the SWE Bench problem if
       aider produced code that wasn't plausibly correct. Plausibly
       correct means that aider reported that it had successfully edited
       the repo without causing syntax errors or breaking any pre-existing
       tests.
     * If the solution from aider with GPT-4o wasn't plausible, the
       harness launched aider to try again from scratch using Claude 3
       Opus.
     * If no plausible solution was found after those two tries, the
       harness picked the "most plausible" solution with the fewest
       edit/lint/test problems.

   It's important to be clear that aider and the benchmark harness only
   had access to the pre-existing tests in each problem's repo. The held
   out "acceptance tests" were only used after benchmarking to compute
   statistics on which problems aider correctly resolved.

   This is the same approach that was used for [92]aider's recent SOTA
   result on SWE Bench Lite. For the Lite benchmark, aider alternated
   between GPT-4o and Opus for up to six total attempts. To manage the
   cost of running the main SWE Bench benchmark, aider was limited to two
   total attempts: one with GPT-4o and one with Opus.

   For a detailed discussion of the benchmark methodology, see the
   [93]article about aider's SWE Bench Lite results. Also, the [94]aider
   SWE Bench repository on GitHub contains the harness and statistics code
   used for the benchmarks.

   The benchmarking process was similar to how a developer might use aider
   to resolve a GitHub issue:
     * They could launch aider in their repo with the command below, which
       tells aider they want to accept every suggestion and to use pytest
       to run tests.
          + aider --yes --test-cmd pytest
     * They could start the chat by pasting in the URL or text of a GitHub
       issue. Aider will pull in the URL's content and then try and
       resolve the issue.
     * If aider doesn't produce code that lints and tests clean, the user
       might decide to [95]use git to revert the changes, and try again
       with aider --opus.

Aider with GPT-4o alone was SOTA

   Using aider with GPT-4o to make a single attempt at resolving each
   problem achieved a score of 17.0%. This was itself a state-of-the-art
   result, before being surpassed by the main result being reported here
   that used aider with both GPT-4o & Opus.

Aider with GPT-4o & Opus

   The benchmark harness started by using aider with GPT-4o to try and
   resolve each problem. For problems where this didn't produce a
   plausible solution, the harness tried again using aider with Opus. So
   at most, two attempts were made for each problem.

   The table below breaks down the proposed solutions that were found from
   each attempt at the 570 problems. A proposed solution is either:
     * A plausible solution where aider reported no outstanding errors
       from editing, linting and testing.
     * Or, the "most plausible" solution generated by either attempt, with
       the [96]fewest outstanding editing, linting or testing errors.

   The table also provides details on the 108 solutions that were
   ultimately verified as correctly resolving their issue.
   Attempt Agent Number of
   proposed
   solutions Percent of
   proposed
   solutions Number of
   correctly
   resolved
   solutions Percent of
   correctly
   resolved
   solutions Score on
   SWE Bench
   Lite
   1 Aider with GPT-4o 419 73.5% 87 80.6% 15.3%
   2 Aider with Opus 151 26.5% 21 19.4% 3.7%
   Total   570 100% 108 100% 18.9%

Non-plausible but correct solutions?

   A solution doesn't actually have to be plausible in order to correctly
   resolve the issue. Recall that plausible is simply defined as aider
   reporting that it successfully completed all file edits, repaired and
   resolved any linting errors and resolved any test failures. But there
   are many reasons why aider might fail to do those things and yet still
   produce a solution that will pass acceptance testing:
     * There may have been pre-existing failing tests in the repo, before
       aider even started working on the SWE Bench problem. Aider may not
       have resolved such issues, and yet they may not be relevant to the
       acceptance testing. The SWE Bench acceptance testing just confirms
       that tests pass or fail in the same pattern as the "gold patch"
       developed by a human to resolve the problem. Some tests may fail
       during acceptance testing, and that's ok as long as they failed for
       the gold patch too.
     * There may have been pre-existing linting problems in the repo. If
       lingering linting issues affected code paths that are not well
       tested, they may not impact acceptance testing.
     * Aider may have reported file editing errors because it thought the
       LLM specified edits that it wasn't able to successfully apply. This
       can only happen when the LLM specified edits in a way that doesn't
       comply with the editing instructions in the system prompt. Given
       that the LLM isn't complying with the system prompt, it may have
       become confused and asked for redundant or otherwise irrelevant
       edits. Such outstanding edit errors might not be fatal for
       acceptance testing.
     * Etc.

   Keeping all this in mind, we can understand why GPT-4o accounts for
   just one attempt of aider with GPT-4o scored 17.0%. When an Opus
   attempt is allowed after GPT-4o, it may propose some incorrect
   solutions which are "more plausible" than some of GPT-4o's
   non-plausible solutions. These more plausible, incorrect solutions can
   eclipse some of the earlier non-plausible correct solutions that GPT-4o
   generated. This is why GPT-4o's score in the table showing the combined
   GPT-4o & Opus results (15.3%) is lower than the result from just one
   try using aider with GPT-4o (17.0%).

   For these reasons, adding additional attempts is not guaranteed to
   monotonically increase the number of resolved problems. New solutions
   may resolve some new problems but they may also eclipse and discard
   some of the previous non-plausible correct solutions.

   Luckily, the net effect of additional attempts usually increases or at
   least maintains the number of resolved solutions. This was the case for
   all the attempts made in both this main SWE Bench result and the
   earlier Lite result.

Computing the benchmark score

   The benchmark harness produced one proposed solution for each of the
   570 SWE Bench problems.

   A separate evaluation script was used to test each of these solutions
   with the full test suite, including the held out acceptance tests. For
   this final acceptance testing, any edits that aider made to tests were
   discarded. This ensured that the correct, unmodified test suite was
   used for acceptance testing. The evaluation script compared each
   proposed solution's test results with results from testing the "gold"
   patch that was developed by a human to correctly resolve the issue. If
   they matched, the proposed solution correctly resolved the issue.

   These acceptance tests were only ever run outside of aider and the
   benchmark harness, and only to compute statistics about the correctly
   resolved instances. They were never run, used, or even visible during
   aider's attempts to resolve the problems.

   Aider correctly resolved 108 out of 570 SWE Bench instances that were
   benchmarked, or 18.9%.

Acknowledgments

   Much thanks to the team behind the [97]SWE Bench family of AI coding
   benchmarks. Also thanks to Albert Örwall who has [98]dockerized the SWE
   Bench evaluation scripts making it faster, easier, and more reliable to
   run the acceptance tests.

