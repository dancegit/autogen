Page: aider.chat_files/aider.chat/docs/more/infinite-output.html
----------------------------------------
   #[1]RSS Feed

   [2]Skip to main content

   [3]aider (BUTTON)

     * [4]Home
     * (BUTTON) [5]Installation
          + [6]Installing aider
          + [7]Optional steps
          + [8]Aider with docker
          + [9]Install with pipx
          + [10]GitHub Codespaces
     * (BUTTON) [11]Usage
          + [12]Tips
          + [13]In-chat commands
          + [14]Chat modes
          + [15]Tutorial videos
          + [16]Voice-to-code with aider
          + [17]Images & web pages
          + [18]Prompt caching
          + [19]Aider in your browser
          + [20]Specifying coding conventions
          + [21]Linting and testing
     * (BUTTON) [22]Connecting to LLMs
          + [23]OpenAI
          + [24]Anthropic
          + [25]Gemini
          + [26]GROQ
          + [27]Azure
          + [28]Cohere
          + [29]DeepSeek
          + [30]Ollama
          + [31]OpenAI compatible APIs
          + [32]OpenRouter
          + [33]Vertex AI
          + [34]Amazon Bedrock
          + [35]Other LLMs
          + [36]Editing format
          + [37]Model warnings
     * (BUTTON) [38]Configuration
          + [39]Options reference
          + [40]YAML config file
          + [41]Config with .env
          + [42]Advanced model settings
     * (BUTTON) [43]Troubleshooting
          + [44]File editing problems
          + [45]Model warnings
          + [46]Token limits
          + [47]Aider not found
          + [48]Dependency versions
          + [49]Using /help
     * (BUTTON) [50]Example chat transcripts
          + [51]Create a simple flask app with aider
          + [52]Modify an open source 2048 game with aider
          + [53]A complex multi-file change, with debugging
          + [54]Create a "black box" test case
          + [55]Automatically update docs with aider
          + [56]Build pong with aider and pygame.
          + [57]Complete a css exercise with aider
          + [58]Download, analyze and plot US Census data
          + [59]Editing an asciinema cast file with aider
          + [60]Hello aider!
          + [61]Honor the NO_COLOR environment variable
          + [62]Improve css styling of chat transcripts
          + [63]Semantic search & replace code with aider
     * (BUTTON) [64]More info
          + [65]Git integration
          + [66]Supported languages
          + [67]Repository map
          + [68]Scripting aider
          + [69]Infinite output
          + [70]Edit formats
          + [71]Analytics
          + [72]Privacy policy
          + [73]Release history
     * [74]FAQ
     * [75]Aider LLM Leaderboards
     * [76]Aider blog

     * [77]GitHub
     * [78]Discord

   Aider is AI pair programming in your terminal. Aider is on [79]GitHub
   and [80]Discord.

   ____________________

     * [81]GitHub
     * [82]Discord
     * [83]Blog


Infinite output

   LLM providers limit how much output a model can generate from a single
   request. This is usually called the output token limit.

   Aider is able to work around this limit with models that support
   "prefilling" the assistant response. When you use aider with a model
   that supports prefill, you will see "infinite output" noted in the
   announcement lines displayed at launch:
Aider v0.58.0
Main model: claude-3-5-sonnet-20240620 with diff edit format, prompt cache, infi
nite output

   Models that support prefill can be primed to think they started their
   response with a specific piece of text. You can put words in their
   mouth, and they will continue generating text from that point forward.

   When aider is collecting code edits from a model and it hits the output
   token limit, aider simply initiates another LLM request with the
   partial response prefilled. This prompts the model to continue where it
   left off, generating more of the desired response. This prefilling of
   the partially completed response can be repeated, allowing for very
   long outputs. Joining the text across these output limit boundaries
   requires some heuristics, but is typically fairly reliable.

   Aider supports "infinite output" for models that support "prefill",
   such as:
     * anthropic.claude-3-5-haiku-20241022-v1:0
     * anthropic.claude-3-5-sonnet-20241022-v2:0
     * anthropic/claude-3-5-sonnet-20241022
     * claude-3-5-haiku-20241022
     * claude-3-5-sonnet-20240620
     * claude-3-5-sonnet-20241022
     * claude-3-haiku-20240307
     * claude-3-opus-20240229
     * claude-3-sonnet-20240229
     * codestral/codestral-2405
     * codestral/codestral-latest
     * deepseek-chat
     * deepseek-coder
     * eu.anthropic.claude-3-5-sonnet-20241022-v2:0
     * mistral/codestral-2405
     * mistral/codestral-latest
     * mistral/codestral-mamba-latest
     * mistral/mistral-large-2402
     * mistral/mistral-large-2407
     * mistral/mistral-large-latest
     * mistral/mistral-medium
     * mistral/mistral-medium-2312
     * mistral/mistral-medium-latest
     * mistral/mistral-small
     * mistral/mistral-small-latest
     * mistral/mistral-tiny
     * mistral/open-codestral-mamba
     * mistral/open-mistral-7b
     * mistral/open-mistral-nemo
     * mistral/open-mistral-nemo-2407
     * mistral/open-mixtral-8x22b
     * mistral/open-mixtral-8x7b
     * mistral/pixtral-12b-2409
     * openrouter/anthropic/claude-3.5-sonnet
     * us.anthropic.claude-3-5-haiku-20241022-v1:0
     * us.anthropic.claude-3-5-sonnet-20241022-v2:0
     * vertex_ai/claude-3-5-haiku@20241022
     * vertex_ai/claude-3-5-sonnet-v2@20241022
     * vertex_ai/claude-3-5-sonnet@20240620
     * vertex_ai/claude-3-haiku@20240307
     * vertex_ai/claude-3-opus@20240229
     * vertex_ai/claude-3-sonnet@20240229

