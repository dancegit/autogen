Page: aider.chat_files/aider.chat/2024/05/22/swe-bench-lite.html
----------------------------------------
   #[1]RSS Feed

   [2]Skip to main content

   [3]aider (BUTTON)

     * [4]Home
     * (BUTTON) [5]Installation
          + [6]Installing aider
          + [7]Optional steps
          + [8]Aider with docker
          + [9]Install with pipx
          + [10]GitHub Codespaces
     * (BUTTON) [11]Usage
          + [12]Tips
          + [13]In-chat commands
          + [14]Chat modes
          + [15]Tutorial videos
          + [16]Voice-to-code with aider
          + [17]Images & web pages
          + [18]Prompt caching
          + [19]Aider in your browser
          + [20]Specifying coding conventions
          + [21]Linting and testing
     * (BUTTON) [22]Connecting to LLMs
          + [23]OpenAI
          + [24]Anthropic
          + [25]Gemini
          + [26]GROQ
          + [27]Azure
          + [28]Cohere
          + [29]DeepSeek
          + [30]Ollama
          + [31]OpenAI compatible APIs
          + [32]OpenRouter
          + [33]Vertex AI
          + [34]Amazon Bedrock
          + [35]Other LLMs
          + [36]Editing format
          + [37]Model warnings
     * (BUTTON) [38]Configuration
          + [39]Options reference
          + [40]YAML config file
          + [41]Config with .env
          + [42]Advanced model settings
     * (BUTTON) [43]Troubleshooting
          + [44]File editing problems
          + [45]Model warnings
          + [46]Token limits
          + [47]Aider not found
          + [48]Dependency versions
          + [49]Using /help
     * (BUTTON) [50]Example chat transcripts
          + [51]Create a simple flask app with aider
          + [52]Modify an open source 2048 game with aider
          + [53]A complex multi-file change, with debugging
          + [54]Create a "black box" test case
          + [55]Automatically update docs with aider
          + [56]Build pong with aider and pygame.
          + [57]Complete a css exercise with aider
          + [58]Download, analyze and plot US Census data
          + [59]Editing an asciinema cast file with aider
          + [60]Hello aider!
          + [61]Honor the NO_COLOR environment variable
          + [62]Improve css styling of chat transcripts
          + [63]Semantic search & replace code with aider
     * (BUTTON) [64]More info
          + [65]Git integration
          + [66]Supported languages
          + [67]Repository map
          + [68]Scripting aider
          + [69]Infinite output
          + [70]Edit formats
          + [71]Analytics
          + [72]Privacy policy
          + [73]Release history
     * [74]FAQ
     * [75]Aider LLM Leaderboards
     * [76]Aider blog

     * [77]GitHub
     * [78]Discord

   Aider is AI pair programming in your terminal. Aider is on [79]GitHub
   and [80]Discord.

   ____________________

     * [81]GitHub
     * [82]Discord
     * [83]Blog

   May 22, 2024

How aider scored SOTA 26.3% on SWE Bench Lite

   [84]Aider scored 26.3% on the [85]SWE Bench Lite benchmark, achieving a
   state-of-the-art result. The previous top leaderboard entry was 20.3%
   from Amazon Q Developer Agent.

   See also [86]aider's SOTA result on the main SWE Bench.

   [87]SWE Bench Lite results

   All of aider's results reported here are pass@1 results, obtained
   without using the SWE Bench hints_text. All results in the above chart
   are unhinted pass@1 results. Please see the [88]references for details
   on the data presented in this chart. It was corrected on 5/30/24 to
   reflect apples-to-apples comparisons, using pass@1 results from
   AutoCodeRover and results from OpenDevin that don't use hints. The
   [89]official SWE Bench Lite leaderboard only accepts pass@1 results
   that do not use hints.

Interactive, not agentic

   Aider achieved this result mainly through its existing features that
   focus on static code analysis, reliable LLM code editing, and pragmatic
   UX for AI pair programming. Aider intentionally has quite limited and
   narrow "agentic behavior" to avoid long delays, high token costs and
   the need for users to repeatedly code review incorrect solutions. It's
   also worth noting that aider currently does not use RAG, vector search,
   tools or give the LLM access to search the web or unilaterally execute
   code.

   Aider is first and foremost an interactive tool for engineers to get
   real work done in real code bases using a chat interface. Aider
   provides a pair programming UX where users can ask for a change and see
   the edits performed in real-time. Aider can also offer additional help
   like fixing lint or test errors, but the user is always in full
   interactive control. This lets them quickly steer misunderstandings
   back on course and avoid wasting time and token costs.

Benchmark methodology

   For the benchmark, aider was launched in each problem's git repository
   with the problem statement submitted as the opening chat message from
   "the user." After that aider runs as normal, with the following
   modifications:
     * Aider's suggestions were always accepted without user approval.
     * A simple harness was used to retry the SWE Bench problem if aider
       produced code that wasn't plausibly correct. Plausibly correct
       means that aider reported that it had successfully edited the repo
       without causing syntax errors or breaking any pre-existing tests.
     * If the solution isn't plausible, the harness launches aider to try
       again from scratch, alternating between using aider with GPT-4o and
       Opus.
     * If no plausible solution is found after six tries, the harness
       picks the solution with the fewest edit/lint/test problems.

   It's important to be clear that aider and the benchmark harness only
   had access to the pre-existing tests in each problem's repo. The held
   out "acceptance tests" were only used after benchmarking to compute
   statistics on which problems aider correctly resolved.

   The [90]full harness to run aider on SWE Bench Lite is available on
   GitHub.

   The benchmarking process was similar to how a developer might use aider
   to resolve a GitHub issue:
     * They could launch aider in their repo with the command below, which
       tells aider they want to accept every suggestion and to use pytest
       to run tests.
          + aider --yes --test-cmd pytest
     * They could start the chat by pasting in the URL or text of a GitHub
       issue. Aider will pull in the URL's content and then try and solve
       the issue.
     * If aider doesn't produce code that lints and tests clean, the user
       might decide to revert the changes and try again, maybe using aider
       with a different LLM this time. [91]Aider is tightly integrated
       with git, so it's always easy to revert AI changes that don't pan
       out.

   Outside a benchmark setting, it's probably unwise or at least highly
   inefficient to let any AI agent run unsupervised on your code base. The
   reason aider is intended to be used interactively is so that the user
   can participate and direct aider's work and approve suggestions. This
   way the user can offer immediate feedback or corrections if their
   initial instructions turn out to be ambiguous, or if the AI starts
   going down a wrong path.

Aider with GPT-4o alone was SOTA

   Running the benchmark harness only using aider with GPT-4o to find
   plausible solutions achieved a score of 25.0%. This was itself matching
   the state-of-the-art, before being surpassed by the main result being
   reported here that used aider with both GPT-4o & Opus.

   As noted below, a single attempt using Aider with GPT-4o tied the
   current top entry on the leaderboard.

Aider with GPT-4o & Opus

   The benchmark harness alternated between running aider with GPT-4o and
   Opus. The harness proceeded in a fixed order, always starting with
   GPT-4o and then alternating with Opus until a plausible solution was
   found for each problem.

   The table below breaks down the plausible solutions that were found for
   the 300 problems. It also provides details on the 79 that were
   ultimately verified as correctly resolving their issue. Some noteworthy
   observations:
     * Just the first attempt of Aider with GPT-4o resolved 20.3% of the
       problems, which ties the Amazon Q Developer Agent currently atop
       the official leaderboard.
     * Including the second attempt, Aider with GPT-4o and Opus scored
       all plausible and ~90% of all resolved solutions.
     * A long tail of solutions continued to be found using both models
       including one correctly resolved solution on the final, sixth
       attempt of that problem.

   Attempt Agent Number of
   plausible
   solutions Percent of
   plausible
   solutions Number of
   correctly
   resolved
   solutions Percent of
   correctly
   resolved
   solutions Score on
   SWE Bench
   Lite
   1 Aider with GPT-4o 208 69.3% 61 77.2% 20.3%
   2 Aider with Opus 49 16.3% 10 12.7% 3.3%
   3 Aider with GPT-4o 20 6.7% 3 3.8% 1.0%
   4 Aider with Opus 9 3.0% 2 2.5% 0.7%
   5 Aider with GPT-4o 11 3.7% 2 2.5% 0.7%
   6 Aider with Opus 3 1.0% 1 1.3% 0.3%
   Total   300 100% 79 100% 26.3%

   If we break down the solutions solely by model, we can see that aider
   with GPT-4o outperforms Opus. This isn't a fair and direct comparison,
   because GPT-4o always took the first turn and therefore got first crack
   at all the "easiest" problems. Aider with Opus only ever saw problems
   that GPT-4o failed to find plausible solutions for on its first try.

   Aider with GPT-4o was producing higher quality plausible solutions,
   with a greater chance of going on to be accepted as resolving the
   issue. Again, this is biased by the turn ordering. But other anecdotal
   evidence from earlier runs of the benchmark also supports the
   observation that aider with GPT-4o is significantly stronger than Opus
   for this benchmark.
   Agent Number of
   plausible
   solutions Number of
   correctly
   resolved
   solutions Percent of
   plausible
   which
   correctly
   resolved
   Aider with GPT-4o 239 66 27.6%
   Aider with Opus 61 13 21.3%
   Total 300 79 26.3%

Repository map, not RAG

   The crucial first step in solving a SWE Bench problem is figuring out
   which parts of the repo are relevant and which files need to be edited.
   Most coding agents use some combination of RAG, vector search and
   providing the LLM with tools to interactively explore the code base.

   Aider instead uses a [92]repository map to help the LLM understand the
   layout, code structure, and content of a git repo. The repo map is
   created through static analysis of the code's abstract syntax tree and
   call graph to provide a compact and powerful summary of the entire code
   base. The map is constantly tailored to show repo context that is
   relevant to the current state of the chat conversation. This is done by
   performing a graph optimization on the code's call graph.

   When the user asks for a change to their code, the LLM can use the repo
   map to decide which files to edit. The LLM simply returns a normal text
   response explaining which files it needs to edit and why. Aider notices
   when the LLM mentions filenames from the repo, and asks the user if
   they should be added to the chat. Adding a file to the chat allows the
   LLM to see the full contents of the file and edit it.

Please add a new /factorial/N endpoint.

   To add a new /factorial/N endpoint, the most likely file that needs to
   be edited is app.py. Please add app.py to the chat so I can proceed
   with the changes.

     app.py
     Add these files to the chat? yes

   This is a convenient and natural workflow for interactive chat, and it
   worked well for the SWE Bench problems. Aider successfully identified
   the correct file to edit in 70.3% of the benchmark tasks.

   We can determine which file needs to be edited using the "gold" patch
   which is associated with each SWE Bench task. This patch was created by
   a human developer to solve the issue, and therefore reveals a file
   which can be edited to solve the problem. Of course aider is not able
   to see or use the gold patch or the file names it contains in any way.
   This information was only used to compute statistics outside the
   benchmarking process.

Reliable code editing

   Once files have been selected for editing, the next step is of course
   to edit the source code to fix the problem.

   Aider goes to great lengths to ensure that LLMs can not just write
   code, but reliably edit code. Aider has a collection of prompting
   strategies and code editing backends which have been honed through
   [93]extensive benchmarking. These foundational capabilities help ensure
   that aider can properly integrate code from LLMs into an existing code
   base and source files.

   The repository map helps here too, making sure that the LLM can see
   relevant classes, functions and variables from the entire repo. This
   helps ensure that the project's existing APIs and conventions are
   respected and utilized when new code is added.

   Regardless, there are still cases where aider may be unable to cleanly
   complete the edits specified by the LLM. This is usually because the
   LLM has failed to conform to the editing instructions in its system
   prompt. When aider completes, it returns an editing outcome that
   indicates whether it was able to successfully apply all edits. The
   benchmark harness uses this editing status as one criteria to determine
   if aider has created a plausible solution.

Linting and fixing

   Another key criteria for a plausible solution is that it passes basic
   linting, which means that the code has no syntax or other fatal errors.
   [94]Aider lints code after every LLM edit and offers to automatically
   fix any problems.

   Aider ships with built-in linters based on tree-sitter which work with
   most popular programming languages. Aider shows linting errors to the
   LLM in a novel format, using the abstract syntax tree to display
   relevant code context for each error. This context helps LLMs
   understand the problem and make the correct changes to resolve it.
app.py:23:36: F821 undefined name 'num'

app.py:
...:3...
  6|class LongNum:
...:3...
 19|    def expound(self, threshold):
 20|        number = self.basis
 21|        while number < threshold:
 22|            number *= self.factor
 23FB        return num
 24|
 25|
...:3...

     Attempt to fix lint errors? yes

   In the benchmark, these linting suggestions are always accepted. At
   completion, aider reports a linting outcome that indicates if it was
   able to produce code without any outstanding linting errors. The
   benchmark harness uses this status as one of the criteria to determine
   if aider has created a plausible solution.

Testing and fixing

   The final crtieria for a plausible solution is that all tests must be
   passing. Aider can be configured with the command to run tests for a
   repo, and will automatically attempt to fix any test failures.

   A user working on a python project might configure testing by launching
   aider like this:
aider --test-cmd pytest

   For the benchmark, aider is configured with a test command that will
   run the tests that already exist in each problem's repository. SWE
   Bench problems are based on repositories from large open source
   projects with extensive existing test suites. This means that testing
   will fail if aider has broken any of these pre-existing tests or if any
   new tests that it created aren't passing.

   As with editing and linting, aider reports a testing outcome that
   indicates if it completed with any outstanding failing tests. The
   benchmark harness uses this status when deciding if aider has produced
   a plausible solution.

   To be clear, aider cannot run or even see the held out "acceptance
   tests" that are used to judge if a proposed solution correctly resolves
   the problem. Those tests are only run outside of aider and the
   benchmark harness, to compute the final benchmark statistics.

Finding a plausible solution

   Each time aider executes, it reports the outcome of the editing,
   linting, and testing steps. Each of these steps may complete
   successfully or return a status that indicates that there were
   outstanding problems that remain unresolved.

   The benchmark harness uses these outcomes to determine if aider has
   produced a plausible solution to the current SWE Bench task. A
   plausible solution is one where aider returns saying that it edited the
   repo with no outstanding edit, lint, or test errors. In this case,
   aider's changes are recorded as the SWE Bench model_patch to be
   evaluated later with the acceptance tests.

   If the solution is not plausible, another instance of aider is launched
   again from scratch on the same problem. The harness alternates
   launching aider with GPT-4o and Opus to solve the problem, and gives
   each model three attempts - for a total of six attempts. As soon as a
   plausible solution is found, it is accepted and the harness moves on to
   the next SWE Bench instance.

   It's worth noting that repositories may have lint or test errors
   present before aider even starts to edit them. Whether unresolved
   errors were caused by aider or were pre-existing, there will be
   instances where no plausible solution is found after six tries.

   If all six attempts fail to produce a plausible solution, then the
   "best" solution available is selected as the model_patch. Which of the
   non-plausible solutions to use is determined by ignoring the testing
   outcome and prioritizing solutions in the following order:
     * Pick a solution where editing and linting were completed
       successfully.
     * Pick a solution where editing was at least partially successful and
       linting succeeded.
     * Pick a solution where editing was successful.
     * Pick a solution where editing was at least partially successful.

Computing the benchmark score

   The benchmark harness produced a plausible solution for each of the 300
   SWE Bench Lite instances and saved it as the model_patch.

   A separate evaluation script was used to test each of these solutions
   with the full test suite, including the held out acceptance tests. For
   this final acceptance testing, any edits that aider made to tests are
   discarded. This ensures that the correct, unmodified test suite is used
   for acceptance testing. The evaluation script compares the test results
   with results from testing the "gold" patch that was developed by a
   human to correctly solve the issue. If they match, the candidate
   solution has correctly resolved the issue.

   These acceptance tests are only ever run outside of aider and the
   benchmark harness, and only to compute the number of correctly resolved
   instances. They are never run, used, or even visible during aider's
   attempts to solve the problems.

   Aider correctly resolved 79 out of 300 SWE Bench Lite instances, or

Acknowledgments

   Much thanks to the team behind the [95]SWE Bench family of AI coding
   benchmarks. Also thanks to Albert Ã–rwall who has [96]dockerized the SWE
   Bench evaluation scripts making it faster, easier, and more reliable to
   run the acceptance tests.

