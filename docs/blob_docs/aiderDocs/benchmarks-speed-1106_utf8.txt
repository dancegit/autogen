Page: aider.chat_files/aider.chat/2023/11/06/benchmarks-speed-1106.html
----------------------------------------
   #[1]RSS Feed

   [2]Skip to main content

   [3]aider (BUTTON)

     * [4]Home
     * (BUTTON) [5]Installation
          + [6]Installing aider
          + [7]Optional steps
          + [8]Aider with docker
          + [9]Install with pipx
          + [10]GitHub Codespaces
     * (BUTTON) [11]Usage
          + [12]Tips
          + [13]In-chat commands
          + [14]Chat modes
          + [15]Tutorial videos
          + [16]Voice-to-code with aider
          + [17]Images & web pages
          + [18]Prompt caching
          + [19]Aider in your browser
          + [20]Specifying coding conventions
          + [21]Linting and testing
     * (BUTTON) [22]Connecting to LLMs
          + [23]OpenAI
          + [24]Anthropic
          + [25]Gemini
          + [26]GROQ
          + [27]Azure
          + [28]Cohere
          + [29]DeepSeek
          + [30]Ollama
          + [31]OpenAI compatible APIs
          + [32]OpenRouter
          + [33]Vertex AI
          + [34]Amazon Bedrock
          + [35]Other LLMs
          + [36]Editing format
          + [37]Model warnings
     * (BUTTON) [38]Configuration
          + [39]Options reference
          + [40]YAML config file
          + [41]Config with .env
          + [42]Advanced model settings
     * (BUTTON) [43]Troubleshooting
          + [44]File editing problems
          + [45]Model warnings
          + [46]Token limits
          + [47]Aider not found
          + [48]Dependency versions
          + [49]Using /help
     * (BUTTON) [50]Example chat transcripts
          + [51]Create a simple flask app with aider
          + [52]Modify an open source 2048 game with aider
          + [53]A complex multi-file change, with debugging
          + [54]Create a "black box" test case
          + [55]Automatically update docs with aider
          + [56]Build pong with aider and pygame.
          + [57]Complete a css exercise with aider
          + [58]Download, analyze and plot US Census data
          + [59]Editing an asciinema cast file with aider
          + [60]Hello aider!
          + [61]Honor the NO_COLOR environment variable
          + [62]Improve css styling of chat transcripts
          + [63]Semantic search & replace code with aider
     * (BUTTON) [64]More info
          + [65]Git integration
          + [66]Supported languages
          + [67]Repository map
          + [68]Scripting aider
          + [69]Infinite output
          + [70]Edit formats
          + [71]Analytics
          + [72]Privacy policy
          + [73]Release history
     * [74]FAQ
     * [75]Aider LLM Leaderboards
     * [76]Aider blog

     * [77]GitHub
     * [78]Discord

   Aider is AI pair programming in your terminal. Aider is on [79]GitHub
   and [80]Discord.

   ____________________

     * [81]GitHub
     * [82]Discord
     * [83]Blog

   November 06, 2023

Speed benchmarks of GPT-4 Turbo and gpt-3.5-turbo-1106

   Nov 6, 2023

   [84]benchmark results

   [85]OpenAI just released new versions of GPT-3.5 and GPT-4, and there's
   a lot of interest about their capabilities and performance. With that
   in mind, I've been benchmarking the new models.

   [86]Aider is an open source command line chat tool that lets you work
   with GPT to edit code in your local git repo. Aider relies on a
   [87]code editing benchmark to quantitatively evaluate performance.

   This is the latest in a series of reports that use the aider
   benchmarking suite to assess and compare the code editing capabilities
   of OpenAI's GPT models. You can review previous reports to get more
   background on aider's benchmark suite:
     * [88]GPT code editing benchmarks evaluates the March and June
       versions of GPT-3.5 and GPT-4.
     * [89]Code editing skill benchmarks for OpenAI's "1106" models
       compares the olders models to the November (1106) models.

Speed

   This report compares the speed of the various GPT models. Aider's
   benchmark measures the response time of the OpenAI chat completion
   endpoint each time it asks GPT to solve a programming exercise in the
   benchmark suite. These results measure only the time spent waiting for
   OpenAI to respond to the prompt. So they are measuring how fast these
   models can generate responses which primarily consist of source code.

   Some observations:
     * GPT-3.5 got 6-11x faster. The gpt-3.5-turbo-1106 model is 6-11x
       faster than the June (0613) version which has been the default
       gpt-3.5-turbo model.
     * GPT-4 Turbo is 2-2.5x faster. The new gpt-4-1106-preview model is
       2-2.5x faster than the June (0613) version which has been the
       default gpt-4 model.
     * The old March (0301) version of GPT-3.5 is actually faster than the
       June (0613) version. This was a surprising discovery.

Updates

   Last updated 11/14/23. OpenAI has relaxed rate limits so these results
   are no longer considered preliminary.

