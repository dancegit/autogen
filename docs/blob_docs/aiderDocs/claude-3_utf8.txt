Page: aider.chat_files/aider.chat/2024/03/08/claude-3.html
----------------------------------------
   #[1]RSS Feed

   [2]Skip to main content

   [3]aider (BUTTON)

     * [4]Home
     * (BUTTON) [5]Installation
          + [6]Installing aider
          + [7]Optional steps
          + [8]Aider with docker
          + [9]Install with pipx
          + [10]GitHub Codespaces
     * (BUTTON) [11]Usage
          + [12]Tips
          + [13]In-chat commands
          + [14]Chat modes
          + [15]Tutorial videos
          + [16]Voice-to-code with aider
          + [17]Images & web pages
          + [18]Prompt caching
          + [19]Aider in your browser
          + [20]Specifying coding conventions
          + [21]Linting and testing
     * (BUTTON) [22]Connecting to LLMs
          + [23]OpenAI
          + [24]Anthropic
          + [25]Gemini
          + [26]GROQ
          + [27]Azure
          + [28]Cohere
          + [29]DeepSeek
          + [30]Ollama
          + [31]OpenAI compatible APIs
          + [32]OpenRouter
          + [33]Vertex AI
          + [34]Amazon Bedrock
          + [35]Other LLMs
          + [36]Editing format
          + [37]Model warnings
     * (BUTTON) [38]Configuration
          + [39]Options reference
          + [40]YAML config file
          + [41]Config with .env
          + [42]Advanced model settings
     * (BUTTON) [43]Troubleshooting
          + [44]File editing problems
          + [45]Model warnings
          + [46]Token limits
          + [47]Aider not found
          + [48]Dependency versions
          + [49]Using /help
     * (BUTTON) [50]Example chat transcripts
          + [51]Create a simple flask app with aider
          + [52]Modify an open source 2048 game with aider
          + [53]A complex multi-file change, with debugging
          + [54]Create a "black box" test case
          + [55]Automatically update docs with aider
          + [56]Build pong with aider and pygame.
          + [57]Complete a css exercise with aider
          + [58]Download, analyze and plot US Census data
          + [59]Editing an asciinema cast file with aider
          + [60]Hello aider!
          + [61]Honor the NO_COLOR environment variable
          + [62]Improve css styling of chat transcripts
          + [63]Semantic search & replace code with aider
     * (BUTTON) [64]More info
          + [65]Git integration
          + [66]Supported languages
          + [67]Repository map
          + [68]Scripting aider
          + [69]Infinite output
          + [70]Edit formats
          + [71]Analytics
          + [72]Privacy policy
          + [73]Release history
     * [74]FAQ
     * [75]Aider LLM Leaderboards
     * [76]Aider blog

     * [77]GitHub
     * [78]Discord

   Aider is AI pair programming in your terminal. Aider is on [79]GitHub
   and [80]Discord.

   ____________________

     * [81]GitHub
     * [82]Discord
     * [83]Blog

   March 08, 2024

Claude 3 beats GPT-4 on Aider's code editing benchmark

   [84]benchmark results

   [85]Anthropic just released their new Claude 3 models with evals
   showing better performance on coding tasks. With that in mind, I've
   been benchmarking the new models using Aider's code editing benchmark
   suite.

   Claude 3 Opus outperforms all of OpenAI's models, making it the best
   available model for pair programming with AI.

   To use Claude 3 Opus with aider:
python -m pip install -U aider-chat
export ANTHROPIC_API_KEY=sk-...
aider --opus

Aider's code editing benchmark

   [86]Aider is an open source command line chat tool that lets you pair
   program with AI on code in your local git repo.

   Aider relies on a [87]code editing benchmark to quantitatively evaluate
   how well an LLM can make changes to existing code. The benchmark uses
   aider to try and complete [88]133 Exercism Python coding exercises. For
   each exercise, Exercism provides a starting python file with stubs for
   the needed functions, a natural language description of the problem to
   solve and a test suite to evaluate whether the coder has correctly
   solved the problem.

   The LLM gets two tries to solve each problem:
       description of the coding task. If the tests all pass, we are done.
       and gives it a second try to complete the task.

Benchmark results

Claude 3 Opus

     * The new claude-3-opus-20240229 model got the highest score ever on
       this benchmark, completing 68.4% of the tasks with two tries.
     * Its single-try performance was comparable to the latest GPT-4 Turbo
       model gpt-4-0125-preview, at 54.1%.
     * While Opus got the highest score, it was only a few points higher
       than the GPT-4 Turbo results. Given the extra costs of Opus and the
       slower response times, it remains to be seen which is the most
       practical model for daily coding use.

Claude 3 Sonnet

     * The new claude-3-sonnet-20240229 model performed similarly to
       OpenAI's GPT-3.5 Turbo models with an overall score of 54.9% and a
       first-try score of 43.6%.

Code editing

   It's highly desirable to have the LLM send back code edits as some form
   of diffs, rather than having it send back an updated copy of the entire
   source code.

   Weaker models like GPT-3.5 are unable to use diffs, and are stuck
   sending back updated copies of entire source files. Aider uses more
   efficient [89]search/replace blocks with the original GPT-4 and
   [90]unified diffs with the newer GPT-4 Turbo models.

   Claude 3 Opus works best with the search/replace blocks, allowing it to
   send back code changes efficiently. Unfortunately, the Sonnet model was
   only able to work reliably with whole files, which limits it to editing
   smaller source files and uses more tokens, money and time.

Other observations

   There are a few other things worth noting:
     * Claude 3 Opus and Sonnet are both slower and more expensive than
       OpenAI's models. You can get almost the same coding skill faster
       and cheaper with OpenAI's models.
     * Claude 3 has a 2X larger context window than the latest GPT-4
       Turbo, which may be an advantage when working with larger code
       bases.
     * The Claude models refused to perform a number of coding tasks and
       returned the error "Output blocked by content filtering policy".
       They refused to code up the [91]beer song program, which makes some
       sort of superficial sense. But they also refused to work in some
       larger open source code bases, for unclear reasons.
     * The Claude APIs seem somewhat unstable, returning HTTP 5xx errors
       of various sorts. Aider automatically recovers from these errors
       with exponential backoff retries, but it's a sign that Anthropic
       made be struggling under surging demand.

